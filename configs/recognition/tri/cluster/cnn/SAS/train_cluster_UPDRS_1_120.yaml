argparse_cfg:
  gpus:
    bind_to: processor_cfg.gpus
    help: number of gpus
  work_dir:
    bind_to: processor_cfg.work_dir
    help: the dir to save logs and models
  batch_size:
    bind_to: processor_cfg.batch_size
  resume_from:
    bind_to: processor_cfg.resume_from
    help: the checkpoint file to resume from


processor_cfg:
  type: 'processor.recognition_tri_cnn.train'
  workers: 1
  test_ids: [34, 37, 27, 47, 39, 46, 48, 50, 52, 55, 57, 59, 66, 33]
  cv: 5
  exclude_cv: False
  notes: "two_part_loss"
  group_notes: "SAS_1_120_bn"
  weight_classes: True
  flip_loss: True
  launch_from_local: False
  wandb_project: "cnn_SAS_v2"
  early_stopping: True
  force_run_all_epochs: False
  es_patience: 15
  es_start_up: 50

  # model setting
  model_cfg:
    type: 'models.backbones.cnn_custom_1'
    in_channels: 3
    num_class: 4
    edge_importance_weighting: True
    input_timesteps: 120
    dropout: 0.0
    data_bn: True
    graph_cfg:
      layout: 'coco_simplified_head'
      strategy: 'spatial'
  loss_cfg:
    # type: 'torch.nn.CrossEntropyLoss'
    # type: 'spacecutter.losses.CumulativeLinkLoss'
    type: 'torch.nn.MSELoss'
    

  # dataset setting
  dataset_cfg:
    # ALL data
    - type: "datasets.DataPipeline"
      data_source:
        type: "datasets.SkeletonLoaderTRI"
        data_dir: skel_data/stgcn_normalized_100_center_all_no_norm_plus_belmont
        num_track: 1
        num_keypoints: 13
        repeat: 1
        outcome_label: SAS_gait
        csv_loader: True
        missing_joint_val: mean
        cache: True
        flip_skels: True

      pipeline:
        - {type: "datasets.skeleton.normalize_by_resolution"}
        - {type: "datasets.skeleton.mask_by_visibility"}
        - {type: "datasets.skeleton.pad_zero_beginning", size: 120 }
        - {type: "datasets.skeleton.random_crop", size: 120 }
        - {type: "datasets.skeleton.transpose", order: [0, 2, 1, 3]}
        - {type: "datasets.skeleton.to_tuple"}

    # PD labelled data
    - type: "datasets.DataPipeline"
      data_source:
        type: "datasets.SkeletonLoaderTRI"
        data_dir: skel_data/stgcn_normalized_100_center_pd_no_norm
        num_track: 1
        num_keypoints: 13
        repeat: 1
        outcome_label: SAS_gait
        csv_loader: True
        missing_joint_val: mean
        cache: True
        flip_skels: True

      pipeline:
        - {type: "datasets.skeleton.normalize_by_resolution"}
        - {type: "datasets.skeleton.mask_by_visibility"}
        - {type: "datasets.skeleton.pad_zero_beginning", size: 120 }
        - {type: "datasets.skeleton.random_crop", size: 120 }
        - {type: "datasets.skeleton.transpose", order: [0, 2, 1, 3]}
        - {type: "datasets.skeleton.to_tuple"}
  # dataloader setting
  batch_size: 40
  gpus: 1

  # optimizer setting
  optimizer_cfg:
    type: 'torch.optim.SGD'
    lr: 0.01
    momentum: 0.9
    nesterov: true
    weight_decay: 0.00001

  # runtime setting
  workflow: [['train', 1], ['val', 1], ['test', 1]]
  work_dir: ./work_dir/recognition/tri_all/dataset_example/cnn/SAS/SAS_1_120_bn
  total_epochs: 150
  training_hooks:
    lr_config:
      policy: 'step'
      step: [50, 100, 120]
    log_config:
      interval: 100
      hooks:
        - type: WandbLoggerHook
    checkpoint_config:
      interval: 200
    optimizer_config:
      grad_clip: 
        max_norm: 5
        norm_type: 2
  resume_from:
  load_from:
